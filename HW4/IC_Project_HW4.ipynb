{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zXjGc8249CU"
      },
      "source": [
        "# Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_Jk0Qoy_4-q2"
      },
      "outputs": [],
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import optimizer\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "from roboticstoolbox import DHRobot, RevoluteMDH \n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv52mOCL5Bw6"
      },
      "source": [
        "# GPU infomation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veGbG3Au5A9h",
        "outputId": "d314254e-e5c3-4047-da6e-fa363b117437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA GeForce RTX 4090\n"
          ]
        }
      ],
      "source": [
        "GPU_name = torch.cuda.get_device_name()\n",
        "print(GPU_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k69yCcbFINMm"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eTQdLFTNIMKV"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    ''' Get device (if GPU is available, use GPU) '''\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def plot_learning_curve(loss_record, title=''):\n",
        "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
        "    total_steps = len(loss_record['train'])\n",
        "    x_1 = range(total_steps)\n",
        "    # x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
        "    x_2 = range(total_steps)\n",
        "    figure(figsize=(6, 4))\n",
        "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
        "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n",
        "    plt.ylim(0.0, 1.)\n",
        "    plt.xlabel('Training steps')\n",
        "    plt.ylabel('MSE loss')\n",
        "    plt.title('Learning curve of {}'.format(title))\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xusm3rMg6XtO"
      },
      "source": [
        "# Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hqeBPyjN6Zc_"
      },
      "outputs": [],
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    ''' A neural network '''\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        # Define your neural network here\n",
        "        # TODO: How to modify this model to achieve better performance?\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.RReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(32, output_dim)\n",
        "        )\n",
        "\n",
        "        # Mean squared error loss\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
        "        return self.net(x)\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        ''' Calculate loss '''\n",
        "        # def euclidean_distance(y_true, y_pred):\n",
        "        # return kr.sqrt(kr.sum(kr.square(y_true - y_pred)))\n",
        "        return self.criterion(pred, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VppnGqq46zB3"
      },
      "source": [
        "# 宣告Dobot位置與軸關節公式函數(順向運動學)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kJvMdPNo6vzX"
      },
      "outputs": [],
      "source": [
        "def dobot_forword_kine(joints):\n",
        "\n",
        "    if joints.ndim == 1:\n",
        "        joints = np.expand_dims(joints, 0)\n",
        "\n",
        "    q1 = joints[:, 0:1] # 0:1而非直接0->確保內部採用Column處理\n",
        "    q2 = joints[:, 1:2]\n",
        "    q3 = joints[:, 2:3]\n",
        "\n",
        "    a2 = 135\n",
        "    a3 = 147\n",
        "    a4 = 61\n",
        "\n",
        "    C1 = np.cos(q1)\n",
        "    C2 = np.cos(q2)\n",
        "    C23 = np.cos(q2 + q3)\n",
        "    S1 = np.sin(q1)\n",
        "    S2 = np.sin(q2)\n",
        "    S23 = np.sin(q2 + q3)\n",
        "\n",
        "    dx = C1 * (a3 * C23 + a2 * C2 + a4)\n",
        "    dy = S1 * (a3 * C23 + a2 * C2 + a4)\n",
        "    dz = -a2 * S2 - a3 * S23\t\n",
        "\n",
        "    Point = np.hstack([dx, dy, dz]) # 建立陣列\n",
        "    return Point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzEiu3uF68g8"
      },
      "source": [
        "# 產生訓練集資料"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IwUOJ5ECHrWg"
      },
      "outputs": [],
      "source": [
        "def gen_data(Train_num):\n",
        "    joint_1 = (-np.pi / 2) + np.pi * np.random.rand(Train_num, 1)\n",
        "    joint_2 = (-85 * np.pi / 180) + (85 * np.pi / 180) * np.random.rand(Train_num, 1)\n",
        "    joint_3 = (-10 * np.pi / 180) + (105 * np.pi / 180) * np.random.rand(Train_num, 1)\n",
        "    joints = np.hstack((joint_1, joint_2, joint_3))\n",
        "    points = dobot_forword_kine(joints)\n",
        "    return points, joints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AU0Czffw6-DS"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = gen_data(10000)\n",
        "x_dev, y_dev = gen_data(500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvWH5qrYhUhC"
      },
      "source": [
        "# Dev Func\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jsIUUqFIhWlV"
      },
      "outputs": [],
      "source": [
        "def dev(feature_dev,target_dev, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    with torch.no_grad():                   # disable gradient calculation\n",
        "        pred = model(feature_dev)                     # forward pass (compute output)\n",
        "        mse_loss = model.cal_loss(pred, target_dev)  # compute loss\n",
        "    return mse_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFfA1n2bicX6"
      },
      "source": [
        "# Train Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so8y3_MjiITW",
        "outputId": "12b97e01-ccbc-41d9-dcdb-0a5fd16c85f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10000, 3])\n",
            "torch.Size([10000, 3])\n",
            "10000\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "# Ref: https://ithelp.ithome.com.tw/articles/10276281\n",
        "\n",
        "# 0) prepare data\n",
        "\n",
        "feature = torch.from_numpy(x_train)\n",
        "print(feature.size())\n",
        "target = torch.from_numpy(y_train)\n",
        "print(target.size())\n",
        "\n",
        "feature_dev = torch.from_numpy(x_dev)\n",
        "target_dev = torch.from_numpy(y_dev)\n",
        "\n",
        "feature,target=feature.type(torch.FloatTensor),target.type(torch.FloatTensor)\n",
        "feature_dev,target_dev=feature_dev.type(torch.FloatTensor),target_dev.type(torch.FloatTensor)\n",
        "\n",
        "\n",
        "n_samples, n_features = feature.shape\n",
        "print(n_samples)\n",
        "print(n_features)\n",
        "\n",
        "# 1) model\n",
        "model = NeuralNet(n_features, n_features)\n",
        "# 2) loss and optimizer\n",
        "learning_rate = 0.001\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3) training loop\n",
        "epochs = 10000\n",
        "min_mse = 1000.0\n",
        "os.makedirs('models', exist_ok=True)\n",
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "loss_record = {'train': [], 'dev': []} "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OheWJG4FdJLJ"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o37HrDabdLfx",
        "outputId": "3d42c845-27ba-4d81-ca63-9763f835cc79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 10, dev_loss =  0.5424\n",
            "Saving model (epoch =   10, train_loss = 0.5429, dev_loss = 0.5424)\n",
            "epoch: 20, dev_loss =  0.3568\n",
            "Saving model (epoch =   20, train_loss = 0.3498, dev_loss = 0.3568)\n",
            "epoch: 30, dev_loss =  0.2507\n",
            "Saving model (epoch =   30, train_loss = 0.2518, dev_loss = 0.2507)\n",
            "epoch: 40, dev_loss =  0.1722\n",
            "Saving model (epoch =   40, train_loss = 0.1685, dev_loss = 0.1722)\n",
            "epoch: 50, dev_loss =  0.1439\n",
            "Saving model (epoch =   50, train_loss = 0.1408, dev_loss = 0.1439)\n",
            "epoch: 60, dev_loss =  0.1240\n",
            "Saving model (epoch =   60, train_loss = 0.1211, dev_loss = 0.1240)\n",
            "epoch: 70, dev_loss =  0.1144\n",
            "Saving model (epoch =   70, train_loss = 0.1094, dev_loss = 0.1144)\n",
            "epoch: 80, dev_loss =  0.1086\n",
            "Saving model (epoch =   80, train_loss = 0.1040, dev_loss = 0.1086)\n",
            "epoch: 90, dev_loss =  0.1046\n",
            "Saving model (epoch =   90, train_loss = 0.1000, dev_loss = 0.1046)\n",
            "epoch: 100, dev_loss =  0.1018\n",
            "Saving model (epoch =  100, train_loss = 0.0973, dev_loss = 0.1018)\n",
            "epoch: 110, dev_loss =  0.0996\n",
            "Saving model (epoch =  110, train_loss = 0.0954, dev_loss = 0.0996)\n",
            "epoch: 120, dev_loss =  0.0975\n",
            "Saving model (epoch =  120, train_loss = 0.0938, dev_loss = 0.0975)\n",
            "epoch: 130, dev_loss =  0.0958\n",
            "Saving model (epoch =  130, train_loss = 0.0926, dev_loss = 0.0958)\n",
            "epoch: 140, dev_loss =  0.0949\n",
            "Saving model (epoch =  140, train_loss = 0.0918, dev_loss = 0.0949)\n",
            "epoch: 150, dev_loss =  0.0942\n",
            "Saving model (epoch =  150, train_loss = 0.0910, dev_loss = 0.0942)\n",
            "epoch: 160, dev_loss =  0.0936\n",
            "Saving model (epoch =  160, train_loss = 0.0905, dev_loss = 0.0936)\n",
            "epoch: 170, dev_loss =  0.0932\n",
            "Saving model (epoch =  170, train_loss = 0.0899, dev_loss = 0.0932)\n",
            "epoch: 180, dev_loss =  0.0926\n",
            "Saving model (epoch =  180, train_loss = 0.0895, dev_loss = 0.0926)\n",
            "epoch: 190, dev_loss =  0.0921\n",
            "Saving model (epoch =  190, train_loss = 0.0891, dev_loss = 0.0921)\n",
            "epoch: 200, dev_loss =  0.0913\n",
            "Saving model (epoch =  200, train_loss = 0.0885, dev_loss = 0.0913)\n",
            "epoch: 210, dev_loss =  0.0906\n",
            "Saving model (epoch =  210, train_loss = 0.0882, dev_loss = 0.0906)\n",
            "epoch: 220, dev_loss =  0.0901\n",
            "Saving model (epoch =  220, train_loss = 0.0879, dev_loss = 0.0901)\n",
            "epoch: 230, dev_loss =  0.0897\n",
            "Saving model (epoch =  230, train_loss = 0.0875, dev_loss = 0.0897)\n",
            "epoch: 240, dev_loss =  0.0894\n",
            "Saving model (epoch =  240, train_loss = 0.0872, dev_loss = 0.0894)\n",
            "epoch: 250, dev_loss =  0.0892\n",
            "Saving model (epoch =  250, train_loss = 0.0869, dev_loss = 0.0892)\n",
            "epoch: 260, dev_loss =  0.0891\n",
            "Saving model (epoch =  260, train_loss = 0.0866, dev_loss = 0.0891)\n",
            "epoch: 270, dev_loss =  0.0890\n",
            "Saving model (epoch =  270, train_loss = 0.0865, dev_loss = 0.0890)\n",
            "epoch: 280, dev_loss =  0.0888\n",
            "Saving model (epoch =  280, train_loss = 0.0862, dev_loss = 0.0888)\n",
            "epoch: 290, dev_loss =  0.0886\n",
            "Saving model (epoch =  290, train_loss = 0.0861, dev_loss = 0.0886)\n",
            "epoch: 300, dev_loss =  0.0883\n",
            "Saving model (epoch =  300, train_loss = 0.0858, dev_loss = 0.0883)\n",
            "epoch: 310, dev_loss =  0.0881\n",
            "Saving model (epoch =  310, train_loss = 0.0856, dev_loss = 0.0881)\n",
            "epoch: 320, dev_loss =  0.0879\n",
            "Saving model (epoch =  320, train_loss = 0.0854, dev_loss = 0.0879)\n",
            "epoch: 330, dev_loss =  0.0878\n",
            "Saving model (epoch =  330, train_loss = 0.0852, dev_loss = 0.0878)\n",
            "epoch: 340, dev_loss =  0.0877\n",
            "Saving model (epoch =  340, train_loss = 0.0850, dev_loss = 0.0877)\n",
            "epoch: 350, dev_loss =  0.0876\n",
            "Saving model (epoch =  350, train_loss = 0.0849, dev_loss = 0.0876)\n",
            "epoch: 360, dev_loss =  0.0875\n",
            "Saving model (epoch =  360, train_loss = 0.0847, dev_loss = 0.0875)\n",
            "epoch: 370, dev_loss =  0.0874\n",
            "Saving model (epoch =  370, train_loss = 0.0845, dev_loss = 0.0874)\n",
            "epoch: 380, dev_loss =  0.0872\n",
            "Saving model (epoch =  380, train_loss = 0.0843, dev_loss = 0.0872)\n",
            "epoch: 390, dev_loss =  0.0869\n",
            "Saving model (epoch =  390, train_loss = 0.0842, dev_loss = 0.0869)\n",
            "epoch: 400, dev_loss =  0.0867\n",
            "Saving model (epoch =  400, train_loss = 0.0839, dev_loss = 0.0867)\n",
            "epoch: 410, dev_loss =  0.0866\n",
            "Saving model (epoch =  410, train_loss = 0.0837, dev_loss = 0.0866)\n",
            "epoch: 420, dev_loss =  0.0865\n",
            "Saving model (epoch =  420, train_loss = 0.0835, dev_loss = 0.0865)\n",
            "epoch: 430, dev_loss =  0.0862\n",
            "Saving model (epoch =  430, train_loss = 0.0833, dev_loss = 0.0862)\n",
            "epoch: 440, dev_loss =  0.0860\n",
            "Saving model (epoch =  440, train_loss = 0.0830, dev_loss = 0.0860)\n",
            "epoch: 450, dev_loss =  0.0858\n",
            "Saving model (epoch =  450, train_loss = 0.0828, dev_loss = 0.0858)\n",
            "epoch: 460, dev_loss =  0.0856\n",
            "Saving model (epoch =  460, train_loss = 0.0825, dev_loss = 0.0856)\n",
            "epoch: 470, dev_loss =  0.0853\n",
            "Saving model (epoch =  470, train_loss = 0.0823, dev_loss = 0.0853)\n",
            "epoch: 480, dev_loss =  0.0850\n",
            "Saving model (epoch =  480, train_loss = 0.0819, dev_loss = 0.0850)\n",
            "epoch: 490, dev_loss =  0.0846\n",
            "Saving model (epoch =  490, train_loss = 0.0816, dev_loss = 0.0846)\n",
            "epoch: 500, dev_loss =  0.0842\n",
            "Saving model (epoch =  500, train_loss = 0.0813, dev_loss = 0.0842)\n",
            "epoch: 510, dev_loss =  0.0838\n",
            "Saving model (epoch =  510, train_loss = 0.0808, dev_loss = 0.0838)\n",
            "epoch: 520, dev_loss =  0.0833\n",
            "Saving model (epoch =  520, train_loss = 0.0802, dev_loss = 0.0833)\n",
            "epoch: 530, dev_loss =  0.0827\n",
            "Saving model (epoch =  530, train_loss = 0.0796, dev_loss = 0.0827)\n",
            "epoch: 540, dev_loss =  0.0817\n",
            "Saving model (epoch =  540, train_loss = 0.0788, dev_loss = 0.0817)\n",
            "epoch: 550, dev_loss =  0.0810\n",
            "Saving model (epoch =  550, train_loss = 0.0780, dev_loss = 0.0810)\n",
            "epoch: 560, dev_loss =  0.0800\n",
            "Saving model (epoch =  560, train_loss = 0.0769, dev_loss = 0.0800)\n",
            "epoch: 570, dev_loss =  0.0791\n",
            "Saving model (epoch =  570, train_loss = 0.0759, dev_loss = 0.0791)\n",
            "epoch: 580, dev_loss =  0.0774\n",
            "Saving model (epoch =  580, train_loss = 0.0746, dev_loss = 0.0774)\n",
            "epoch: 590, dev_loss =  0.0761\n",
            "Saving model (epoch =  590, train_loss = 0.0732, dev_loss = 0.0761)\n",
            "epoch: 600, dev_loss =  0.0750\n",
            "Saving model (epoch =  600, train_loss = 0.0716, dev_loss = 0.0750)\n",
            "epoch: 610, dev_loss =  0.0732\n",
            "Saving model (epoch =  610, train_loss = 0.0707, dev_loss = 0.0732)\n",
            "epoch: 620, dev_loss =  0.0716\n",
            "Saving model (epoch =  620, train_loss = 0.0686, dev_loss = 0.0716)\n",
            "epoch: 630, dev_loss =  0.0702\n",
            "Saving model (epoch =  630, train_loss = 0.0668, dev_loss = 0.0702)\n",
            "epoch: 640, dev_loss =  0.0693\n",
            "Saving model (epoch =  640, train_loss = 0.0672, dev_loss = 0.0693)\n",
            "epoch: 650, dev_loss =  0.0681\n",
            "Saving model (epoch =  650, train_loss = 0.0641, dev_loss = 0.0681)\n",
            "epoch: 660, dev_loss =  0.0670\n",
            "Saving model (epoch =  660, train_loss = 0.0626, dev_loss = 0.0670)\n",
            "epoch: 670, dev_loss =  0.0653\n",
            "Saving model (epoch =  670, train_loss = 0.0612, dev_loss = 0.0653)\n",
            "epoch: 680, dev_loss =  0.0632\n",
            "Saving model (epoch =  680, train_loss = 0.0596, dev_loss = 0.0632)\n",
            "epoch: 690, dev_loss =  0.0688\n",
            "epoch: 700, dev_loss =  0.0649\n",
            "epoch: 710, dev_loss =  0.0618\n",
            "Saving model (epoch =  710, train_loss = 0.0565, dev_loss = 0.0618)\n",
            "epoch: 720, dev_loss =  0.0599\n",
            "Saving model (epoch =  720, train_loss = 0.0557, dev_loss = 0.0599)\n",
            "epoch: 730, dev_loss =  0.0593\n",
            "Saving model (epoch =  730, train_loss = 0.0546, dev_loss = 0.0593)\n",
            "epoch: 740, dev_loss =  0.0585\n",
            "Saving model (epoch =  740, train_loss = 0.0536, dev_loss = 0.0585)\n",
            "epoch: 750, dev_loss =  0.0580\n",
            "Saving model (epoch =  750, train_loss = 0.0528, dev_loss = 0.0580)\n",
            "epoch: 760, dev_loss =  0.0582\n",
            "epoch: 770, dev_loss =  0.0579\n",
            "Saving model (epoch =  770, train_loss = 0.0550, dev_loss = 0.0579)\n",
            "epoch: 780, dev_loss =  0.0575\n",
            "Saving model (epoch =  780, train_loss = 0.0516, dev_loss = 0.0575)\n",
            "epoch: 790, dev_loss =  0.0557\n",
            "Saving model (epoch =  790, train_loss = 0.0500, dev_loss = 0.0557)\n",
            "epoch: 800, dev_loss =  0.0550\n",
            "Saving model (epoch =  800, train_loss = 0.0491, dev_loss = 0.0550)\n",
            "epoch: 810, dev_loss =  0.0545\n",
            "Saving model (epoch =  810, train_loss = 0.0483, dev_loss = 0.0545)\n",
            "epoch: 820, dev_loss =  0.0539\n",
            "Saving model (epoch =  820, train_loss = 0.0477, dev_loss = 0.0539)\n",
            "epoch: 830, dev_loss =  0.0576\n",
            "epoch: 840, dev_loss =  0.0540\n",
            "epoch: 850, dev_loss =  0.0529\n",
            "Saving model (epoch =  850, train_loss = 0.0460, dev_loss = 0.0529)\n",
            "epoch: 860, dev_loss =  0.0520\n",
            "Saving model (epoch =  860, train_loss = 0.0456, dev_loss = 0.0520)\n",
            "epoch: 870, dev_loss =  0.0514\n",
            "Saving model (epoch =  870, train_loss = 0.0449, dev_loss = 0.0514)\n",
            "epoch: 880, dev_loss =  0.0512\n",
            "Saving model (epoch =  880, train_loss = 0.0442, dev_loss = 0.0512)\n",
            "epoch: 890, dev_loss =  0.0550\n",
            "epoch: 900, dev_loss =  0.0507\n",
            "Saving model (epoch =  900, train_loss = 0.0452, dev_loss = 0.0507)\n",
            "epoch: 910, dev_loss =  0.0498\n",
            "Saving model (epoch =  910, train_loss = 0.0435, dev_loss = 0.0498)\n",
            "epoch: 920, dev_loss =  0.0493\n",
            "Saving model (epoch =  920, train_loss = 0.0427, dev_loss = 0.0493)\n",
            "epoch: 930, dev_loss =  0.0494\n",
            "epoch: 940, dev_loss =  0.0516\n",
            "epoch: 950, dev_loss =  0.0484\n",
            "Saving model (epoch =  950, train_loss = 0.0411, dev_loss = 0.0484)\n",
            "epoch: 960, dev_loss =  0.0494\n",
            "epoch: 970, dev_loss =  0.0498\n",
            "epoch: 980, dev_loss =  0.0473\n",
            "Saving model (epoch =  980, train_loss = 0.0404, dev_loss = 0.0473)\n",
            "epoch: 990, dev_loss =  0.0471\n",
            "Saving model (epoch =  990, train_loss = 0.0398, dev_loss = 0.0471)\n",
            "epoch: 1000, dev_loss =  0.0465\n",
            "Saving model (epoch = 1000, train_loss = 0.0395, dev_loss = 0.0465)\n",
            "epoch: 1010, dev_loss =  0.0487\n",
            "epoch: 1020, dev_loss =  0.0487\n",
            "epoch: 1030, dev_loss =  0.0476\n",
            "epoch: 1040, dev_loss =  0.0459\n",
            "Saving model (epoch = 1040, train_loss = 0.0382, dev_loss = 0.0459)\n",
            "epoch: 1050, dev_loss =  0.0453\n",
            "Saving model (epoch = 1050, train_loss = 0.0380, dev_loss = 0.0453)\n",
            "epoch: 1060, dev_loss =  0.0449\n",
            "Saving model (epoch = 1060, train_loss = 0.0374, dev_loss = 0.0449)\n",
            "epoch: 1070, dev_loss =  0.0449\n",
            "Saving model (epoch = 1070, train_loss = 0.0373, dev_loss = 0.0449)\n",
            "epoch: 1080, dev_loss =  0.0483\n",
            "epoch: 1090, dev_loss =  0.0464\n",
            "epoch: 1100, dev_loss =  0.0451\n",
            "epoch: 1110, dev_loss =  0.0446\n",
            "Saving model (epoch = 1110, train_loss = 0.0362, dev_loss = 0.0446)\n",
            "epoch: 1120, dev_loss =  0.0444\n",
            "Saving model (epoch = 1120, train_loss = 0.0362, dev_loss = 0.0444)\n",
            "epoch: 1130, dev_loss =  0.0441\n",
            "Saving model (epoch = 1130, train_loss = 0.0358, dev_loss = 0.0441)\n",
            "epoch: 1140, dev_loss =  0.0444\n",
            "epoch: 1150, dev_loss =  0.0453\n",
            "epoch: 1160, dev_loss =  0.0438\n",
            "Saving model (epoch = 1160, train_loss = 0.0355, dev_loss = 0.0438)\n",
            "epoch: 1170, dev_loss =  0.0437\n",
            "Saving model (epoch = 1170, train_loss = 0.0359, dev_loss = 0.0437)\n",
            "epoch: 1180, dev_loss =  0.0432\n",
            "Saving model (epoch = 1180, train_loss = 0.0348, dev_loss = 0.0432)\n",
            "epoch: 1190, dev_loss =  0.0431\n",
            "Saving model (epoch = 1190, train_loss = 0.0346, dev_loss = 0.0431)\n",
            "epoch: 1200, dev_loss =  0.0492\n",
            "epoch: 1210, dev_loss =  0.0428\n",
            "Saving model (epoch = 1210, train_loss = 0.0354, dev_loss = 0.0428)\n",
            "epoch: 1220, dev_loss =  0.0439\n",
            "epoch: 1230, dev_loss =  0.0426\n",
            "Saving model (epoch = 1230, train_loss = 0.0342, dev_loss = 0.0426)\n",
            "epoch: 1240, dev_loss =  0.0424\n",
            "Saving model (epoch = 1240, train_loss = 0.0336, dev_loss = 0.0424)\n",
            "epoch: 1250, dev_loss =  0.0422\n",
            "Saving model (epoch = 1250, train_loss = 0.0336, dev_loss = 0.0422)\n",
            "epoch: 1260, dev_loss =  0.0416\n",
            "Saving model (epoch = 1260, train_loss = 0.0334, dev_loss = 0.0416)\n",
            "epoch: 1270, dev_loss =  0.0420\n",
            "epoch: 1280, dev_loss =  0.0414\n",
            "Saving model (epoch = 1280, train_loss = 0.0330, dev_loss = 0.0414)\n",
            "epoch: 1290, dev_loss =  0.0409\n",
            "Saving model (epoch = 1290, train_loss = 0.0329, dev_loss = 0.0409)\n",
            "epoch: 1300, dev_loss =  0.0423\n",
            "epoch: 1310, dev_loss =  0.0430\n",
            "epoch: 1320, dev_loss =  0.0417\n",
            "epoch: 1330, dev_loss =  0.0406\n",
            "Saving model (epoch = 1330, train_loss = 0.0327, dev_loss = 0.0406)\n",
            "epoch: 1340, dev_loss =  0.0417\n",
            "epoch: 1350, dev_loss =  0.0410\n",
            "epoch: 1360, dev_loss =  0.0404\n",
            "Saving model (epoch = 1360, train_loss = 0.0321, dev_loss = 0.0404)\n",
            "epoch: 1370, dev_loss =  0.0397\n",
            "Saving model (epoch = 1370, train_loss = 0.0317, dev_loss = 0.0397)\n",
            "epoch: 1380, dev_loss =  0.0414\n",
            "epoch: 1390, dev_loss =  0.0417\n",
            "epoch: 1400, dev_loss =  0.0392\n",
            "Saving model (epoch = 1400, train_loss = 0.0316, dev_loss = 0.0392)\n",
            "epoch: 1410, dev_loss =  0.0390\n",
            "Saving model (epoch = 1410, train_loss = 0.0314, dev_loss = 0.0390)\n",
            "epoch: 1420, dev_loss =  0.0392\n",
            "epoch: 1430, dev_loss =  0.0388\n",
            "Saving model (epoch = 1430, train_loss = 0.0312, dev_loss = 0.0388)\n",
            "epoch: 1440, dev_loss =  0.0386\n",
            "Saving model (epoch = 1440, train_loss = 0.0308, dev_loss = 0.0386)\n",
            "epoch: 1450, dev_loss =  0.0384\n",
            "Saving model (epoch = 1450, train_loss = 0.0308, dev_loss = 0.0384)\n",
            "epoch: 1460, dev_loss =  0.0418\n",
            "epoch: 1470, dev_loss =  0.0382\n",
            "Saving model (epoch = 1470, train_loss = 0.0308, dev_loss = 0.0382)\n",
            "epoch: 1480, dev_loss =  0.0382\n",
            "Saving model (epoch = 1480, train_loss = 0.0310, dev_loss = 0.0382)\n",
            "epoch: 1490, dev_loss =  0.0379\n",
            "Saving model (epoch = 1490, train_loss = 0.0310, dev_loss = 0.0379)\n",
            "epoch: 1500, dev_loss =  0.0375\n",
            "Saving model (epoch = 1500, train_loss = 0.0303, dev_loss = 0.0375)\n",
            "epoch: 1510, dev_loss =  0.0375\n",
            "Saving model (epoch = 1510, train_loss = 0.0303, dev_loss = 0.0375)\n",
            "epoch: 1520, dev_loss =  0.0373\n",
            "Saving model (epoch = 1520, train_loss = 0.0300, dev_loss = 0.0373)\n",
            "epoch: 1530, dev_loss =  0.0479\n",
            "epoch: 1540, dev_loss =  0.0378\n",
            "epoch: 1550, dev_loss =  0.0389\n",
            "epoch: 1560, dev_loss =  0.0368\n",
            "Saving model (epoch = 1560, train_loss = 0.0300, dev_loss = 0.0368)\n",
            "epoch: 1570, dev_loss =  0.0369\n",
            "epoch: 1580, dev_loss =  0.0360\n",
            "Saving model (epoch = 1580, train_loss = 0.0295, dev_loss = 0.0360)\n",
            "epoch: 1590, dev_loss =  0.0359\n",
            "Saving model (epoch = 1590, train_loss = 0.0292, dev_loss = 0.0359)\n",
            "epoch: 1600, dev_loss =  0.0368\n",
            "epoch: 1610, dev_loss =  0.0353\n",
            "Saving model (epoch = 1610, train_loss = 0.0292, dev_loss = 0.0353)\n",
            "epoch: 1620, dev_loss =  0.0352\n",
            "Saving model (epoch = 1620, train_loss = 0.0294, dev_loss = 0.0352)\n",
            "epoch: 1630, dev_loss =  0.0364\n",
            "epoch: 1640, dev_loss =  0.0350\n",
            "Saving model (epoch = 1640, train_loss = 0.0286, dev_loss = 0.0350)\n",
            "epoch: 1650, dev_loss =  0.0345\n",
            "Saving model (epoch = 1650, train_loss = 0.0283, dev_loss = 0.0345)\n",
            "epoch: 1660, dev_loss =  0.0439\n",
            "epoch: 1670, dev_loss =  0.0350\n",
            "epoch: 1680, dev_loss =  0.0343\n",
            "Saving model (epoch = 1680, train_loss = 0.0295, dev_loss = 0.0343)\n",
            "epoch: 1690, dev_loss =  0.0346\n",
            "epoch: 1700, dev_loss =  0.0335\n",
            "Saving model (epoch = 1700, train_loss = 0.0279, dev_loss = 0.0335)\n",
            "epoch: 1710, dev_loss =  0.0339\n",
            "epoch: 1720, dev_loss =  0.0331\n",
            "Saving model (epoch = 1720, train_loss = 0.0273, dev_loss = 0.0331)\n",
            "epoch: 1730, dev_loss =  0.0331\n",
            "Saving model (epoch = 1730, train_loss = 0.0269, dev_loss = 0.0331)\n",
            "epoch: 1740, dev_loss =  0.0344\n",
            "epoch: 1750, dev_loss =  0.0335\n",
            "epoch: 1760, dev_loss =  0.0326\n",
            "Saving model (epoch = 1760, train_loss = 0.0269, dev_loss = 0.0326)\n",
            "epoch: 1770, dev_loss =  0.0321\n",
            "Saving model (epoch = 1770, train_loss = 0.0263, dev_loss = 0.0321)\n",
            "epoch: 1780, dev_loss =  0.0320\n",
            "Saving model (epoch = 1780, train_loss = 0.0263, dev_loss = 0.0320)\n",
            "epoch: 1790, dev_loss =  0.0453\n",
            "epoch: 1800, dev_loss =  0.0357\n",
            "epoch: 1810, dev_loss =  0.0318\n",
            "Saving model (epoch = 1810, train_loss = 0.0270, dev_loss = 0.0318)\n",
            "epoch: 1820, dev_loss =  0.0313\n",
            "Saving model (epoch = 1820, train_loss = 0.0258, dev_loss = 0.0313)\n",
            "epoch: 1830, dev_loss =  0.0311\n",
            "Saving model (epoch = 1830, train_loss = 0.0253, dev_loss = 0.0311)\n",
            "epoch: 1840, dev_loss =  0.0308\n",
            "Saving model (epoch = 1840, train_loss = 0.0251, dev_loss = 0.0308)\n",
            "epoch: 1850, dev_loss =  0.0299\n",
            "Saving model (epoch = 1850, train_loss = 0.0249, dev_loss = 0.0299)\n",
            "epoch: 1860, dev_loss =  0.0294\n",
            "Saving model (epoch = 1860, train_loss = 0.0249, dev_loss = 0.0294)\n",
            "epoch: 1870, dev_loss =  0.0297\n",
            "epoch: 1880, dev_loss =  0.0315\n",
            "epoch: 1890, dev_loss =  0.0291\n",
            "Saving model (epoch = 1890, train_loss = 0.0259, dev_loss = 0.0291)\n",
            "epoch: 1900, dev_loss =  0.0286\n",
            "Saving model (epoch = 1900, train_loss = 0.0244, dev_loss = 0.0286)\n",
            "epoch: 1910, dev_loss =  0.0289\n",
            "epoch: 1920, dev_loss =  0.0335\n",
            "epoch: 1930, dev_loss =  0.0293\n",
            "epoch: 1940, dev_loss =  0.0279\n",
            "Saving model (epoch = 1940, train_loss = 0.0239, dev_loss = 0.0279)\n",
            "epoch: 1950, dev_loss =  0.0274\n",
            "Saving model (epoch = 1950, train_loss = 0.0233, dev_loss = 0.0274)\n",
            "epoch: 1960, dev_loss =  0.0325\n",
            "epoch: 1970, dev_loss =  0.0275\n",
            "epoch: 1980, dev_loss =  0.0272\n",
            "Saving model (epoch = 1980, train_loss = 0.0241, dev_loss = 0.0272)\n",
            "epoch: 1990, dev_loss =  0.0268\n",
            "Saving model (epoch = 1990, train_loss = 0.0230, dev_loss = 0.0268)\n",
            "epoch: 2000, dev_loss =  0.0270\n",
            "epoch: 2010, dev_loss =  0.0321\n",
            "epoch: 2020, dev_loss =  0.0309\n",
            "epoch: 2030, dev_loss =  0.0264\n",
            "Saving model (epoch = 2030, train_loss = 0.0219, dev_loss = 0.0264)\n",
            "epoch: 2040, dev_loss =  0.0259\n",
            "Saving model (epoch = 2040, train_loss = 0.0224, dev_loss = 0.0259)\n",
            "epoch: 2050, dev_loss =  0.0253\n",
            "Saving model (epoch = 2050, train_loss = 0.0217, dev_loss = 0.0253)\n",
            "epoch: 2060, dev_loss =  0.0252\n",
            "Saving model (epoch = 2060, train_loss = 0.0222, dev_loss = 0.0252)\n",
            "epoch: 2070, dev_loss =  0.0268\n",
            "epoch: 2080, dev_loss =  0.0255\n",
            "epoch: 2090, dev_loss =  0.0269\n",
            "epoch: 2100, dev_loss =  0.0246\n",
            "Saving model (epoch = 2100, train_loss = 0.0211, dev_loss = 0.0246)\n",
            "epoch: 2110, dev_loss =  0.0239\n",
            "Saving model (epoch = 2110, train_loss = 0.0208, dev_loss = 0.0239)\n",
            "epoch: 2120, dev_loss =  0.0247\n",
            "epoch: 2130, dev_loss =  0.0256\n",
            "epoch: 2140, dev_loss =  0.0235\n",
            "Saving model (epoch = 2140, train_loss = 0.0206, dev_loss = 0.0235)\n",
            "epoch: 2150, dev_loss =  0.0244\n",
            "epoch: 2160, dev_loss =  0.0242\n",
            "epoch: 2170, dev_loss =  0.0252\n",
            "epoch: 2180, dev_loss =  0.0251\n",
            "epoch: 2190, dev_loss =  0.0237\n",
            "epoch: 2200, dev_loss =  0.0221\n",
            "Saving model (epoch = 2200, train_loss = 0.0192, dev_loss = 0.0221)\n",
            "epoch: 2210, dev_loss =  0.0219\n",
            "Saving model (epoch = 2210, train_loss = 0.0191, dev_loss = 0.0219)\n",
            "epoch: 2220, dev_loss =  0.0308\n",
            "epoch: 2230, dev_loss =  0.0244\n",
            "epoch: 2240, dev_loss =  0.0218\n",
            "Saving model (epoch = 2240, train_loss = 0.0189, dev_loss = 0.0218)\n",
            "epoch: 2250, dev_loss =  0.0212\n",
            "Saving model (epoch = 2250, train_loss = 0.0186, dev_loss = 0.0212)\n",
            "epoch: 2260, dev_loss =  0.0216\n",
            "epoch: 2270, dev_loss =  0.0395\n",
            "epoch: 2280, dev_loss =  0.0251\n",
            "epoch: 2290, dev_loss =  0.0225\n",
            "epoch: 2300, dev_loss =  0.0212\n",
            "epoch: 2310, dev_loss =  0.0203\n",
            "Saving model (epoch = 2310, train_loss = 0.0177, dev_loss = 0.0203)\n",
            "epoch: 2320, dev_loss =  0.0201\n",
            "Saving model (epoch = 2320, train_loss = 0.0176, dev_loss = 0.0201)\n",
            "epoch: 2330, dev_loss =  0.0203\n",
            "epoch: 2340, dev_loss =  0.0240\n",
            "epoch: 2350, dev_loss =  0.0204\n",
            "epoch: 2360, dev_loss =  0.0369\n",
            "epoch: 2370, dev_loss =  0.0211\n",
            "epoch: 2380, dev_loss =  0.0225\n",
            "epoch: 2390, dev_loss =  0.0193\n",
            "Saving model (epoch = 2390, train_loss = 0.0179, dev_loss = 0.0193)\n",
            "epoch: 2400, dev_loss =  0.0192\n",
            "Saving model (epoch = 2400, train_loss = 0.0167, dev_loss = 0.0192)\n",
            "epoch: 2410, dev_loss =  0.0190\n",
            "Saving model (epoch = 2410, train_loss = 0.0165, dev_loss = 0.0190)\n",
            "epoch: 2420, dev_loss =  0.0187\n",
            "Saving model (epoch = 2420, train_loss = 0.0165, dev_loss = 0.0187)\n",
            "epoch: 2430, dev_loss =  0.0185\n",
            "Saving model (epoch = 2430, train_loss = 0.0162, dev_loss = 0.0185)\n",
            "epoch: 2440, dev_loss =  0.0185\n",
            "epoch: 2450, dev_loss =  0.0181\n",
            "Saving model (epoch = 2450, train_loss = 0.0159, dev_loss = 0.0181)\n",
            "epoch: 2460, dev_loss =  0.0257\n",
            "epoch: 2470, dev_loss =  0.0259\n",
            "epoch: 2480, dev_loss =  0.0196\n",
            "epoch: 2490, dev_loss =  0.0184\n",
            "epoch: 2500, dev_loss =  0.0176\n",
            "Saving model (epoch = 2500, train_loss = 0.0155, dev_loss = 0.0176)\n",
            "epoch: 2510, dev_loss =  0.0173\n",
            "Saving model (epoch = 2510, train_loss = 0.0154, dev_loss = 0.0173)\n",
            "epoch: 2520, dev_loss =  0.0171\n",
            "Saving model (epoch = 2520, train_loss = 0.0153, dev_loss = 0.0171)\n",
            "epoch: 2530, dev_loss =  0.0233\n",
            "epoch: 2540, dev_loss =  0.0221\n",
            "epoch: 2550, dev_loss =  0.0187\n",
            "epoch: 2560, dev_loss =  0.0170\n",
            "Saving model (epoch = 2560, train_loss = 0.0156, dev_loss = 0.0170)\n",
            "epoch: 2570, dev_loss =  0.0168\n",
            "Saving model (epoch = 2570, train_loss = 0.0150, dev_loss = 0.0168)\n",
            "epoch: 2580, dev_loss =  0.0165\n",
            "Saving model (epoch = 2580, train_loss = 0.0147, dev_loss = 0.0165)\n",
            "epoch: 2590, dev_loss =  0.0386\n",
            "epoch: 2600, dev_loss =  0.0178\n",
            "epoch: 2610, dev_loss =  0.0171\n",
            "epoch: 2620, dev_loss =  0.0163\n",
            "Saving model (epoch = 2620, train_loss = 0.0145, dev_loss = 0.0163)\n",
            "epoch: 2630, dev_loss =  0.0164\n",
            "epoch: 2640, dev_loss =  0.0158\n",
            "Saving model (epoch = 2640, train_loss = 0.0143, dev_loss = 0.0158)\n",
            "epoch: 2650, dev_loss =  0.0157\n",
            "Saving model (epoch = 2650, train_loss = 0.0142, dev_loss = 0.0157)\n",
            "epoch: 2660, dev_loss =  0.0155\n",
            "Saving model (epoch = 2660, train_loss = 0.0140, dev_loss = 0.0155)\n",
            "epoch: 2670, dev_loss =  0.0158\n",
            "epoch: 2680, dev_loss =  0.0384\n",
            "epoch: 2690, dev_loss =  0.0165\n",
            "epoch: 2700, dev_loss =  0.0160\n",
            "epoch: 2710, dev_loss =  0.0155\n",
            "Saving model (epoch = 2710, train_loss = 0.0141, dev_loss = 0.0155)\n",
            "epoch: 2720, dev_loss =  0.0150\n",
            "Saving model (epoch = 2720, train_loss = 0.0138, dev_loss = 0.0150)\n",
            "epoch: 2730, dev_loss =  0.0153\n",
            "epoch: 2740, dev_loss =  0.0148\n",
            "Saving model (epoch = 2740, train_loss = 0.0137, dev_loss = 0.0148)\n",
            "epoch: 2750, dev_loss =  0.0149\n",
            "epoch: 2760, dev_loss =  0.0159\n",
            "epoch: 2770, dev_loss =  0.0199\n",
            "epoch: 2780, dev_loss =  0.0152\n",
            "epoch: 2790, dev_loss =  0.0153\n",
            "epoch: 2800, dev_loss =  0.0145\n",
            "Saving model (epoch = 2800, train_loss = 0.0134, dev_loss = 0.0145)\n",
            "epoch: 2810, dev_loss =  0.0151\n",
            "epoch: 2820, dev_loss =  0.0140\n",
            "Saving model (epoch = 2820, train_loss = 0.0127, dev_loss = 0.0140)\n",
            "epoch: 2830, dev_loss =  0.0254\n",
            "epoch: 2840, dev_loss =  0.0194\n",
            "epoch: 2850, dev_loss =  0.0141\n",
            "epoch: 2860, dev_loss =  0.0143\n",
            "epoch: 2870, dev_loss =  0.0139\n",
            "Saving model (epoch = 2870, train_loss = 0.0126, dev_loss = 0.0139)\n",
            "epoch: 2880, dev_loss =  0.0136\n",
            "Saving model (epoch = 2880, train_loss = 0.0125, dev_loss = 0.0136)\n",
            "epoch: 2890, dev_loss =  0.0136\n",
            "epoch: 2900, dev_loss =  0.0143\n",
            "epoch: 2910, dev_loss =  0.0260\n",
            "epoch: 2920, dev_loss =  0.0148\n",
            "epoch: 2930, dev_loss =  0.0152\n",
            "epoch: 2940, dev_loss =  0.0144\n",
            "epoch: 2950, dev_loss =  0.0134\n",
            "Saving model (epoch = 2950, train_loss = 0.0125, dev_loss = 0.0134)\n",
            "epoch: 2960, dev_loss =  0.0134\n",
            "Saving model (epoch = 2960, train_loss = 0.0120, dev_loss = 0.0134)\n",
            "epoch: 2970, dev_loss =  0.0131\n",
            "Saving model (epoch = 2970, train_loss = 0.0119, dev_loss = 0.0131)\n",
            "epoch: 2980, dev_loss =  0.0131\n",
            "Saving model (epoch = 2980, train_loss = 0.0119, dev_loss = 0.0131)\n",
            "epoch: 2990, dev_loss =  0.0146\n",
            "epoch: 3000, dev_loss =  0.0138\n",
            "epoch: 3010, dev_loss =  0.0132\n",
            "epoch: 3020, dev_loss =  0.0134\n",
            "epoch: 3030, dev_loss =  0.0130\n",
            "Saving model (epoch = 3030, train_loss = 0.0117, dev_loss = 0.0130)\n",
            "epoch: 3040, dev_loss =  0.0237\n",
            "epoch: 3050, dev_loss =  0.0161\n",
            "epoch: 3060, dev_loss =  0.0127\n",
            "Saving model (epoch = 3060, train_loss = 0.0122, dev_loss = 0.0127)\n",
            "epoch: 3070, dev_loss =  0.0126\n",
            "Saving model (epoch = 3070, train_loss = 0.0113, dev_loss = 0.0126)\n",
            "epoch: 3080, dev_loss =  0.0126\n",
            "Saving model (epoch = 3080, train_loss = 0.0117, dev_loss = 0.0126)\n",
            "epoch: 3090, dev_loss =  0.0124\n",
            "Saving model (epoch = 3090, train_loss = 0.0116, dev_loss = 0.0124)\n",
            "epoch: 3100, dev_loss =  0.0130\n",
            "epoch: 3110, dev_loss =  0.0249\n",
            "epoch: 3120, dev_loss =  0.0148\n",
            "epoch: 3130, dev_loss =  0.0128\n",
            "epoch: 3140, dev_loss =  0.0121\n",
            "Saving model (epoch = 3140, train_loss = 0.0111, dev_loss = 0.0121)\n",
            "epoch: 3150, dev_loss =  0.0123\n",
            "epoch: 3160, dev_loss =  0.0124\n",
            "epoch: 3170, dev_loss =  0.0158\n",
            "epoch: 3180, dev_loss =  0.0127\n",
            "epoch: 3190, dev_loss =  0.0118\n",
            "Saving model (epoch = 3190, train_loss = 0.0112, dev_loss = 0.0118)\n",
            "epoch: 3200, dev_loss =  0.0121\n",
            "epoch: 3210, dev_loss =  0.0117\n",
            "Saving model (epoch = 3210, train_loss = 0.0108, dev_loss = 0.0117)\n",
            "epoch: 3220, dev_loss =  0.0126\n",
            "epoch: 3230, dev_loss =  0.0197\n",
            "epoch: 3240, dev_loss =  0.0132\n",
            "epoch: 3250, dev_loss =  0.0121\n",
            "epoch: 3260, dev_loss =  0.0125\n",
            "epoch: 3270, dev_loss =  0.0125\n",
            "epoch: 3280, dev_loss =  0.0114\n",
            "Saving model (epoch = 3280, train_loss = 0.0109, dev_loss = 0.0114)\n",
            "epoch: 3290, dev_loss =  0.0115\n",
            "epoch: 3300, dev_loss =  0.0116\n",
            "epoch: 3310, dev_loss =  0.0114\n",
            "Saving model (epoch = 3310, train_loss = 0.0106, dev_loss = 0.0114)\n",
            "epoch: 3320, dev_loss =  0.0122\n",
            "epoch: 3330, dev_loss =  0.0119\n",
            "epoch: 3340, dev_loss =  0.0117\n",
            "epoch: 3350, dev_loss =  0.0160\n",
            "epoch: 3360, dev_loss =  0.0151\n",
            "epoch: 3370, dev_loss =  0.0117\n",
            "epoch: 3380, dev_loss =  0.0113\n",
            "Saving model (epoch = 3380, train_loss = 0.0114, dev_loss = 0.0113)\n",
            "epoch: 3390, dev_loss =  0.0114\n",
            "epoch: 3400, dev_loss =  0.0113\n",
            "epoch: 3410, dev_loss =  0.0111\n",
            "Saving model (epoch = 3410, train_loss = 0.0103, dev_loss = 0.0111)\n",
            "epoch: 3420, dev_loss =  0.0119\n",
            "epoch: 3430, dev_loss =  0.0155\n",
            "epoch: 3440, dev_loss =  0.0125\n",
            "epoch: 3450, dev_loss =  0.0125\n",
            "epoch: 3460, dev_loss =  0.0124\n",
            "epoch: 3470, dev_loss =  0.0114\n",
            "epoch: 3480, dev_loss =  0.0179\n",
            "epoch: 3490, dev_loss =  0.0161\n",
            "epoch: 3500, dev_loss =  0.0110\n",
            "Saving model (epoch = 3500, train_loss = 0.0105, dev_loss = 0.0110)\n",
            "epoch: 3510, dev_loss =  0.0110\n",
            "epoch: 3520, dev_loss =  0.0111\n",
            "epoch: 3530, dev_loss =  0.0107\n",
            "Saving model (epoch = 3530, train_loss = 0.0100, dev_loss = 0.0107)\n",
            "epoch: 3540, dev_loss =  0.0115\n",
            "epoch: 3550, dev_loss =  0.0108\n",
            "epoch: 3560, dev_loss =  0.0107\n",
            "epoch: 3570, dev_loss =  0.0111\n",
            "epoch: 3580, dev_loss =  0.0318\n",
            "epoch: 3590, dev_loss =  0.0119\n",
            "epoch: 3600, dev_loss =  0.0107\n",
            "Saving model (epoch = 3600, train_loss = 0.0100, dev_loss = 0.0107)\n",
            "epoch: 3610, dev_loss =  0.0109\n",
            "epoch: 3620, dev_loss =  0.0107\n",
            "epoch: 3630, dev_loss =  0.0104\n",
            "Saving model (epoch = 3630, train_loss = 0.0099, dev_loss = 0.0104)\n",
            "epoch: 3640, dev_loss =  0.0104\n",
            "Saving model (epoch = 3640, train_loss = 0.0096, dev_loss = 0.0104)\n",
            "epoch: 3650, dev_loss =  0.0108\n",
            "epoch: 3660, dev_loss =  0.0164\n",
            "epoch: 3670, dev_loss =  0.0106\n",
            "epoch: 3680, dev_loss =  0.0110\n",
            "epoch: 3690, dev_loss =  0.0109\n",
            "epoch: 3700, dev_loss =  0.0214\n",
            "epoch: 3710, dev_loss =  0.0159\n",
            "epoch: 3720, dev_loss =  0.0114\n",
            "epoch: 3730, dev_loss =  0.0105\n",
            "epoch: 3740, dev_loss =  0.0102\n",
            "Saving model (epoch = 3740, train_loss = 0.0095, dev_loss = 0.0102)\n",
            "epoch: 3750, dev_loss =  0.0102\n",
            "Saving model (epoch = 3750, train_loss = 0.0094, dev_loss = 0.0102)\n",
            "epoch: 3760, dev_loss =  0.0103\n",
            "epoch: 3770, dev_loss =  0.0101\n",
            "Saving model (epoch = 3770, train_loss = 0.0093, dev_loss = 0.0101)\n",
            "epoch: 3780, dev_loss =  0.0101\n",
            "Saving model (epoch = 3780, train_loss = 0.0092, dev_loss = 0.0101)\n",
            "epoch: 3790, dev_loss =  0.0276\n",
            "epoch: 3800, dev_loss =  0.0114\n",
            "epoch: 3810, dev_loss =  0.0124\n",
            "epoch: 3820, dev_loss =  0.0118\n",
            "epoch: 3830, dev_loss =  0.0111\n",
            "epoch: 3840, dev_loss =  0.0101\n",
            "epoch: 3850, dev_loss =  0.0100\n",
            "Saving model (epoch = 3850, train_loss = 0.0093, dev_loss = 0.0100)\n",
            "epoch: 3860, dev_loss =  0.0100\n",
            "Saving model (epoch = 3860, train_loss = 0.0092, dev_loss = 0.0100)\n",
            "epoch: 3870, dev_loss =  0.0102\n",
            "epoch: 3880, dev_loss =  0.0099\n",
            "Saving model (epoch = 3880, train_loss = 0.0091, dev_loss = 0.0099)\n",
            "epoch: 3890, dev_loss =  0.0098\n",
            "Saving model (epoch = 3890, train_loss = 0.0091, dev_loss = 0.0098)\n",
            "epoch: 3900, dev_loss =  0.0098\n",
            "Saving model (epoch = 3900, train_loss = 0.0091, dev_loss = 0.0098)\n",
            "epoch: 3910, dev_loss =  0.0124\n",
            "epoch: 3920, dev_loss =  0.0157\n",
            "epoch: 3930, dev_loss =  0.0107\n",
            "epoch: 3940, dev_loss =  0.0100\n",
            "epoch: 3950, dev_loss =  0.0102\n",
            "epoch: 3960, dev_loss =  0.0105\n",
            "epoch: 3970, dev_loss =  0.0225\n",
            "epoch: 3980, dev_loss =  0.0111\n",
            "epoch: 3990, dev_loss =  0.0108\n",
            "epoch: 4000, dev_loss =  0.0105\n",
            "epoch: 4010, dev_loss =  0.0099\n",
            "epoch: 4020, dev_loss =  0.0097\n",
            "Saving model (epoch = 4020, train_loss = 0.0088, dev_loss = 0.0097)\n",
            "epoch: 4030, dev_loss =  0.0098\n",
            "epoch: 4040, dev_loss =  0.0121\n",
            "epoch: 4050, dev_loss =  0.0123\n",
            "epoch: 4060, dev_loss =  0.0107\n",
            "epoch: 4070, dev_loss =  0.0110\n",
            "epoch: 4080, dev_loss =  0.0099\n",
            "epoch: 4090, dev_loss =  0.0097\n",
            "Saving model (epoch = 4090, train_loss = 0.0088, dev_loss = 0.0097)\n",
            "epoch: 4100, dev_loss =  0.0095\n",
            "Saving model (epoch = 4100, train_loss = 0.0087, dev_loss = 0.0095)\n",
            "epoch: 4110, dev_loss =  0.0094\n",
            "Saving model (epoch = 4110, train_loss = 0.0087, dev_loss = 0.0094)\n",
            "epoch: 4120, dev_loss =  0.0101\n",
            "epoch: 4130, dev_loss =  0.0272\n",
            "epoch: 4140, dev_loss =  0.0130\n",
            "epoch: 4150, dev_loss =  0.0104\n",
            "epoch: 4160, dev_loss =  0.0096\n",
            "epoch: 4170, dev_loss =  0.0096\n",
            "epoch: 4180, dev_loss =  0.0094\n",
            "Saving model (epoch = 4180, train_loss = 0.0087, dev_loss = 0.0094)\n",
            "epoch: 4190, dev_loss =  0.0095\n",
            "epoch: 4200, dev_loss =  0.0097\n",
            "epoch: 4210, dev_loss =  0.0096\n",
            "epoch: 4220, dev_loss =  0.0117\n",
            "epoch: 4230, dev_loss =  0.0103\n",
            "epoch: 4240, dev_loss =  0.0099\n",
            "epoch: 4250, dev_loss =  0.0100\n",
            "epoch: 4260, dev_loss =  0.0112\n",
            "epoch: 4270, dev_loss =  0.0103\n",
            "epoch: 4280, dev_loss =  0.0093\n",
            "Saving model (epoch = 4280, train_loss = 0.0085, dev_loss = 0.0093)\n",
            "epoch: 4290, dev_loss =  0.0125\n",
            "epoch: 4300, dev_loss =  0.0126\n",
            "epoch: 4310, dev_loss =  0.0125\n",
            "epoch: 4320, dev_loss =  0.0096\n",
            "epoch: 4330, dev_loss =  0.0091\n",
            "Saving model (epoch = 4330, train_loss = 0.0087, dev_loss = 0.0091)\n",
            "epoch: 4340, dev_loss =  0.0091\n",
            "epoch: 4350, dev_loss =  0.0091\n",
            "epoch: 4360, dev_loss =  0.0096\n",
            "epoch: 4370, dev_loss =  0.0186\n",
            "epoch: 4380, dev_loss =  0.0115\n",
            "epoch: 4390, dev_loss =  0.0102\n",
            "epoch: 4400, dev_loss =  0.0096\n",
            "epoch: 4410, dev_loss =  0.0099\n",
            "epoch: 4420, dev_loss =  0.0096\n",
            "epoch: 4430, dev_loss =  0.0180\n",
            "epoch: 4440, dev_loss =  0.0096\n",
            "epoch: 4450, dev_loss =  0.0091\n",
            "epoch: 4460, dev_loss =  0.0094\n",
            "epoch: 4470, dev_loss =  0.0089\n",
            "Saving model (epoch = 4470, train_loss = 0.0081, dev_loss = 0.0089)\n",
            "epoch: 4480, dev_loss =  0.0093\n",
            "epoch: 4490, dev_loss =  0.0117\n",
            "epoch: 4500, dev_loss =  0.0154\n",
            "epoch: 4510, dev_loss =  0.0095\n",
            "epoch: 4520, dev_loss =  0.0102\n",
            "epoch: 4530, dev_loss =  0.0089\n",
            "epoch: 4540, dev_loss =  0.0087\n",
            "Saving model (epoch = 4540, train_loss = 0.0081, dev_loss = 0.0087)\n",
            "epoch: 4550, dev_loss =  0.0091\n",
            "epoch: 4560, dev_loss =  0.0176\n",
            "epoch: 4570, dev_loss =  0.0132\n",
            "epoch: 4580, dev_loss =  0.0095\n",
            "epoch: 4590, dev_loss =  0.0088\n",
            "epoch: 4600, dev_loss =  0.0087\n",
            "Saving model (epoch = 4600, train_loss = 0.0082, dev_loss = 0.0087)\n",
            "epoch: 4610, dev_loss =  0.0095\n",
            "epoch: 4620, dev_loss =  0.0138\n",
            "epoch: 4630, dev_loss =  0.0091\n",
            "epoch: 4640, dev_loss =  0.0092\n",
            "epoch: 4650, dev_loss =  0.0113\n",
            "epoch: 4660, dev_loss =  0.0096\n",
            "epoch: 4670, dev_loss =  0.0114\n",
            "epoch: 4680, dev_loss =  0.0085\n",
            "Saving model (epoch = 4680, train_loss = 0.0080, dev_loss = 0.0085)\n",
            "epoch: 4690, dev_loss =  0.0107\n",
            "epoch: 4700, dev_loss =  0.0104\n",
            "epoch: 4710, dev_loss =  0.0094\n",
            "epoch: 4720, dev_loss =  0.0089\n",
            "epoch: 4730, dev_loss =  0.0097\n",
            "epoch: 4740, dev_loss =  0.0120\n",
            "epoch: 4750, dev_loss =  0.0087\n",
            "epoch: 4760, dev_loss =  0.0112\n",
            "epoch: 4770, dev_loss =  0.0093\n",
            "epoch: 4780, dev_loss =  0.0103\n",
            "epoch: 4790, dev_loss =  0.0099\n",
            "epoch: 4800, dev_loss =  0.0091\n",
            "epoch: 4810, dev_loss =  0.0111\n",
            "epoch: 4820, dev_loss =  0.0087\n",
            "epoch: 4830, dev_loss =  0.0102\n",
            "epoch: 4840, dev_loss =  0.0090\n",
            "epoch: 4850, dev_loss =  0.0084\n",
            "Saving model (epoch = 4850, train_loss = 0.0077, dev_loss = 0.0084)\n",
            "epoch: 4860, dev_loss =  0.0136\n",
            "epoch: 4870, dev_loss =  0.0138\n",
            "epoch: 4880, dev_loss =  0.0091\n",
            "epoch: 4890, dev_loss =  0.0094\n",
            "epoch: 4900, dev_loss =  0.0089\n",
            "epoch: 4910, dev_loss =  0.0083\n",
            "Saving model (epoch = 4910, train_loss = 0.0078, dev_loss = 0.0083)\n",
            "epoch: 4920, dev_loss =  0.0084\n",
            "epoch: 4930, dev_loss =  0.0092\n",
            "epoch: 4940, dev_loss =  0.0107\n",
            "epoch: 4950, dev_loss =  0.0087\n",
            "epoch: 4960, dev_loss =  0.0135\n",
            "epoch: 4970, dev_loss =  0.0130\n",
            "epoch: 4980, dev_loss =  0.0083\n",
            "epoch: 4990, dev_loss =  0.0097\n",
            "epoch: 5000, dev_loss =  0.0083\n",
            "epoch: 5010, dev_loss =  0.0085\n",
            "epoch: 5020, dev_loss =  0.0089\n",
            "epoch: 5030, dev_loss =  0.0089\n",
            "epoch: 5040, dev_loss =  0.0148\n",
            "epoch: 5050, dev_loss =  0.0089\n",
            "epoch: 5060, dev_loss =  0.0087\n",
            "epoch: 5070, dev_loss =  0.0083\n",
            "epoch: 5080, dev_loss =  0.0184\n",
            "epoch: 5090, dev_loss =  0.0131\n",
            "epoch: 5100, dev_loss =  0.0082\n",
            "Saving model (epoch = 5100, train_loss = 0.0084, dev_loss = 0.0082)\n",
            "epoch: 5110, dev_loss =  0.0085\n",
            "epoch: 5120, dev_loss =  0.0081\n",
            "Saving model (epoch = 5120, train_loss = 0.0077, dev_loss = 0.0081)\n",
            "epoch: 5130, dev_loss =  0.0084\n",
            "epoch: 5140, dev_loss =  0.0085\n",
            "epoch: 5150, dev_loss =  0.0137\n",
            "epoch: 5160, dev_loss =  0.0118\n",
            "epoch: 5170, dev_loss =  0.0092\n",
            "epoch: 5180, dev_loss =  0.0089\n",
            "epoch: 5190, dev_loss =  0.0184\n",
            "epoch: 5200, dev_loss =  0.0089\n",
            "epoch: 5210, dev_loss =  0.0083\n",
            "epoch: 5220, dev_loss =  0.0081\n",
            "Saving model (epoch = 5220, train_loss = 0.0074, dev_loss = 0.0081)\n",
            "epoch: 5230, dev_loss =  0.0080\n",
            "Saving model (epoch = 5230, train_loss = 0.0074, dev_loss = 0.0080)\n",
            "epoch: 5240, dev_loss =  0.0083\n",
            "epoch: 5250, dev_loss =  0.0115\n",
            "epoch: 5260, dev_loss =  0.0100\n",
            "epoch: 5270, dev_loss =  0.0098\n",
            "epoch: 5280, dev_loss =  0.0092\n",
            "epoch: 5290, dev_loss =  0.0095\n",
            "epoch: 5300, dev_loss =  0.0080\n",
            "epoch: 5310, dev_loss =  0.0086\n",
            "epoch: 5320, dev_loss =  0.0195\n",
            "epoch: 5330, dev_loss =  0.0105\n",
            "epoch: 5340, dev_loss =  0.0088\n",
            "epoch: 5350, dev_loss =  0.0085\n",
            "epoch: 5360, dev_loss =  0.0078\n",
            "Saving model (epoch = 5360, train_loss = 0.0073, dev_loss = 0.0078)\n",
            "epoch: 5370, dev_loss =  0.0091\n",
            "epoch: 5380, dev_loss =  0.0170\n",
            "epoch: 5390, dev_loss =  0.0085\n",
            "epoch: 5400, dev_loss =  0.0086\n",
            "epoch: 5410, dev_loss =  0.0081\n",
            "epoch: 5420, dev_loss =  0.0085\n",
            "epoch: 5430, dev_loss =  0.0079\n",
            "epoch: 5440, dev_loss =  0.0080\n",
            "epoch: 5450, dev_loss =  0.0241\n",
            "epoch: 5460, dev_loss =  0.0081\n",
            "epoch: 5470, dev_loss =  0.0106\n",
            "epoch: 5480, dev_loss =  0.0080\n",
            "epoch: 5490, dev_loss =  0.0079\n",
            "epoch: 5500, dev_loss =  0.0079\n",
            "epoch: 5510, dev_loss =  0.0078\n",
            "Saving model (epoch = 5510, train_loss = 0.0072, dev_loss = 0.0078)\n",
            "epoch: 5520, dev_loss =  0.0093\n",
            "epoch: 5530, dev_loss =  0.0080\n",
            "epoch: 5540, dev_loss =  0.0078\n",
            "Saving model (epoch = 5540, train_loss = 0.0071, dev_loss = 0.0078)\n",
            "epoch: 5550, dev_loss =  0.0086\n",
            "epoch: 5560, dev_loss =  0.0099\n",
            "epoch: 5570, dev_loss =  0.0108\n",
            "epoch: 5580, dev_loss =  0.0107\n",
            "epoch: 5590, dev_loss =  0.0083\n",
            "epoch: 5600, dev_loss =  0.0082\n",
            "epoch: 5610, dev_loss =  0.0077\n",
            "Saving model (epoch = 5610, train_loss = 0.0073, dev_loss = 0.0077)\n",
            "epoch: 5620, dev_loss =  0.0084\n",
            "epoch: 5630, dev_loss =  0.0076\n",
            "Saving model (epoch = 5630, train_loss = 0.0070, dev_loss = 0.0076)\n",
            "epoch: 5640, dev_loss =  0.0081\n",
            "epoch: 5650, dev_loss =  0.0128\n",
            "epoch: 5660, dev_loss =  0.0089\n",
            "epoch: 5670, dev_loss =  0.0080\n",
            "epoch: 5680, dev_loss =  0.0085\n",
            "epoch: 5690, dev_loss =  0.0101\n",
            "epoch: 5700, dev_loss =  0.0085\n",
            "epoch: 5710, dev_loss =  0.0088\n",
            "epoch: 5720, dev_loss =  0.0094\n",
            "epoch: 5730, dev_loss =  0.0096\n",
            "epoch: 5740, dev_loss =  0.0077\n",
            "epoch: 5750, dev_loss =  0.0076\n",
            "Saving model (epoch = 5750, train_loss = 0.0069, dev_loss = 0.0076)\n",
            "epoch: 5760, dev_loss =  0.0141\n",
            "epoch: 5770, dev_loss =  0.0124\n",
            "epoch: 5780, dev_loss =  0.0090\n",
            "epoch: 5790, dev_loss =  0.0083\n",
            "epoch: 5800, dev_loss =  0.0078\n",
            "epoch: 5810, dev_loss =  0.0076\n",
            "epoch: 5820, dev_loss =  0.0086\n",
            "epoch: 5830, dev_loss =  0.0117\n",
            "epoch: 5840, dev_loss =  0.0075\n",
            "Saving model (epoch = 5840, train_loss = 0.0070, dev_loss = 0.0075)\n",
            "epoch: 5850, dev_loss =  0.0092\n",
            "epoch: 5860, dev_loss =  0.0105\n",
            "epoch: 5870, dev_loss =  0.0093\n",
            "epoch: 5880, dev_loss =  0.0086\n",
            "epoch: 5890, dev_loss =  0.0083\n",
            "epoch: 5900, dev_loss =  0.0093\n",
            "epoch: 5910, dev_loss =  0.0088\n",
            "epoch: 5920, dev_loss =  0.0075\n",
            "epoch: 5930, dev_loss =  0.0146\n",
            "epoch: 5940, dev_loss =  0.0117\n",
            "epoch: 5950, dev_loss =  0.0075\n",
            "Saving model (epoch = 5950, train_loss = 0.0070, dev_loss = 0.0075)\n",
            "epoch: 5960, dev_loss =  0.0087\n",
            "epoch: 5970, dev_loss =  0.0097\n",
            "epoch: 5980, dev_loss =  0.0090\n",
            "epoch: 5990, dev_loss =  0.0079\n",
            "epoch: 6000, dev_loss =  0.0076\n",
            "epoch: 6010, dev_loss =  0.0172\n",
            "epoch: 6020, dev_loss =  0.0107\n",
            "epoch: 6030, dev_loss =  0.0082\n",
            "epoch: 6040, dev_loss =  0.0080\n",
            "epoch: 6050, dev_loss =  0.0073\n",
            "Saving model (epoch = 6050, train_loss = 0.0066, dev_loss = 0.0073)\n",
            "epoch: 6060, dev_loss =  0.0077\n",
            "epoch: 6070, dev_loss =  0.0170\n",
            "epoch: 6080, dev_loss =  0.0099\n",
            "epoch: 6090, dev_loss =  0.0091\n",
            "epoch: 6100, dev_loss =  0.0082\n",
            "epoch: 6110, dev_loss =  0.0073\n",
            "Saving model (epoch = 6110, train_loss = 0.0067, dev_loss = 0.0073)\n",
            "epoch: 6120, dev_loss =  0.0079\n",
            "epoch: 6130, dev_loss =  0.0117\n",
            "epoch: 6140, dev_loss =  0.0077\n",
            "epoch: 6150, dev_loss =  0.0079\n",
            "epoch: 6160, dev_loss =  0.0083\n",
            "epoch: 6170, dev_loss =  0.0082\n",
            "epoch: 6180, dev_loss =  0.0077\n",
            "epoch: 6190, dev_loss =  0.0080\n",
            "epoch: 6200, dev_loss =  0.0193\n",
            "epoch: 6210, dev_loss =  0.0083\n",
            "epoch: 6220, dev_loss =  0.0074\n",
            "epoch: 6230, dev_loss =  0.0075\n",
            "epoch: 6240, dev_loss =  0.0078\n",
            "epoch: 6250, dev_loss =  0.0072\n",
            "Saving model (epoch = 6250, train_loss = 0.0066, dev_loss = 0.0072)\n",
            "epoch: 6260, dev_loss =  0.0091\n",
            "epoch: 6270, dev_loss =  0.0103\n",
            "epoch: 6280, dev_loss =  0.0083\n",
            "epoch: 6290, dev_loss =  0.0132\n",
            "epoch: 6300, dev_loss =  0.0092\n",
            "epoch: 6310, dev_loss =  0.0090\n",
            "epoch: 6320, dev_loss =  0.0072\n",
            "Saving model (epoch = 6320, train_loss = 0.0066, dev_loss = 0.0072)\n",
            "epoch: 6330, dev_loss =  0.0072\n",
            "epoch: 6340, dev_loss =  0.0076\n",
            "epoch: 6350, dev_loss =  0.0089\n",
            "epoch: 6360, dev_loss =  0.0117\n",
            "epoch: 6370, dev_loss =  0.0075\n",
            "epoch: 6380, dev_loss =  0.0071\n",
            "Saving model (epoch = 6380, train_loss = 0.0067, dev_loss = 0.0071)\n",
            "epoch: 6390, dev_loss =  0.0073\n",
            "epoch: 6400, dev_loss =  0.0081\n",
            "epoch: 6410, dev_loss =  0.0098\n",
            "epoch: 6420, dev_loss =  0.0071\n",
            "Saving model (epoch = 6420, train_loss = 0.0065, dev_loss = 0.0071)\n",
            "epoch: 6430, dev_loss =  0.0099\n",
            "epoch: 6440, dev_loss =  0.0104\n",
            "epoch: 6450, dev_loss =  0.0081\n",
            "epoch: 6460, dev_loss =  0.0099\n",
            "epoch: 6470, dev_loss =  0.0074\n",
            "epoch: 6480, dev_loss =  0.0083\n",
            "epoch: 6490, dev_loss =  0.0126\n",
            "epoch: 6500, dev_loss =  0.0073\n",
            "epoch: 6510, dev_loss =  0.0072\n",
            "epoch: 6520, dev_loss =  0.0072\n",
            "epoch: 6530, dev_loss =  0.0070\n",
            "Saving model (epoch = 6530, train_loss = 0.0065, dev_loss = 0.0070)\n",
            "epoch: 6540, dev_loss =  0.0088\n",
            "epoch: 6550, dev_loss =  0.0072\n",
            "epoch: 6560, dev_loss =  0.0227\n",
            "epoch: 6570, dev_loss =  0.0085\n",
            "epoch: 6580, dev_loss =  0.0094\n",
            "epoch: 6590, dev_loss =  0.0081\n",
            "epoch: 6600, dev_loss =  0.0073\n",
            "epoch: 6610, dev_loss =  0.0079\n",
            "epoch: 6620, dev_loss =  0.0070\n",
            "epoch: 6630, dev_loss =  0.0090\n",
            "epoch: 6640, dev_loss =  0.0081\n",
            "epoch: 6650, dev_loss =  0.0072\n",
            "epoch: 6660, dev_loss =  0.0096\n",
            "epoch: 6670, dev_loss =  0.0076\n",
            "epoch: 6680, dev_loss =  0.0098\n",
            "epoch: 6690, dev_loss =  0.0086\n",
            "epoch: 6700, dev_loss =  0.0074\n",
            "epoch: 6710, dev_loss =  0.0069\n",
            "Saving model (epoch = 6710, train_loss = 0.0063, dev_loss = 0.0069)\n",
            "epoch: 6720, dev_loss =  0.0070\n",
            "epoch: 6730, dev_loss =  0.0071\n",
            "epoch: 6740, dev_loss =  0.0188\n",
            "epoch: 6750, dev_loss =  0.0083\n",
            "epoch: 6760, dev_loss =  0.0072\n",
            "epoch: 6770, dev_loss =  0.0072\n",
            "epoch: 6780, dev_loss =  0.0069\n",
            "Saving model (epoch = 6780, train_loss = 0.0065, dev_loss = 0.0069)\n",
            "epoch: 6790, dev_loss =  0.0072\n",
            "epoch: 6800, dev_loss =  0.0097\n",
            "epoch: 6810, dev_loss =  0.0083\n",
            "epoch: 6820, dev_loss =  0.0071\n",
            "epoch: 6830, dev_loss =  0.0096\n",
            "epoch: 6840, dev_loss =  0.0115\n",
            "epoch: 6850, dev_loss =  0.0071\n",
            "epoch: 6860, dev_loss =  0.0102\n",
            "epoch: 6870, dev_loss =  0.0072\n",
            "epoch: 6880, dev_loss =  0.0078\n",
            "epoch: 6890, dev_loss =  0.0075\n",
            "epoch: 6900, dev_loss =  0.0080\n",
            "epoch: 6910, dev_loss =  0.0122\n",
            "epoch: 6920, dev_loss =  0.0092\n",
            "epoch: 6930, dev_loss =  0.0073\n",
            "epoch: 6940, dev_loss =  0.0074\n",
            "epoch: 6950, dev_loss =  0.0072\n",
            "epoch: 6960, dev_loss =  0.0072\n",
            "epoch: 6970, dev_loss =  0.0121\n",
            "epoch: 6980, dev_loss =  0.0087\n",
            "epoch: 6990, dev_loss =  0.0071\n",
            "epoch: 7000, dev_loss =  0.0069\n",
            "Saving model (epoch = 7000, train_loss = 0.0062, dev_loss = 0.0069)\n",
            "epoch: 7010, dev_loss =  0.0087\n",
            "epoch: 7020, dev_loss =  0.0077\n",
            "epoch: 7030, dev_loss =  0.0079\n",
            "epoch: 7040, dev_loss =  0.0072\n",
            "epoch: 7050, dev_loss =  0.0068\n",
            "Saving model (epoch = 7050, train_loss = 0.0062, dev_loss = 0.0068)\n",
            "epoch: 7060, dev_loss =  0.0070\n",
            "epoch: 7070, dev_loss =  0.0075\n",
            "epoch: 7080, dev_loss =  0.0125\n",
            "epoch: 7090, dev_loss =  0.0113\n",
            "epoch: 7100, dev_loss =  0.0069\n",
            "epoch: 7110, dev_loss =  0.0081\n",
            "epoch: 7120, dev_loss =  0.0070\n",
            "epoch: 7130, dev_loss =  0.0121\n",
            "epoch: 7140, dev_loss =  0.0077\n",
            "epoch: 7150, dev_loss =  0.0068\n",
            "Saving model (epoch = 7150, train_loss = 0.0061, dev_loss = 0.0068)\n",
            "epoch: 7160, dev_loss =  0.0084\n",
            "epoch: 7170, dev_loss =  0.0079\n",
            "epoch: 7180, dev_loss =  0.0066\n",
            "Saving model (epoch = 7180, train_loss = 0.0060, dev_loss = 0.0066)\n",
            "epoch: 7190, dev_loss =  0.0081\n",
            "epoch: 7200, dev_loss =  0.0163\n",
            "epoch: 7210, dev_loss =  0.0079\n",
            "epoch: 7220, dev_loss =  0.0073\n",
            "epoch: 7230, dev_loss =  0.0068\n",
            "epoch: 7240, dev_loss =  0.0068\n",
            "epoch: 7250, dev_loss =  0.0066\n",
            "Saving model (epoch = 7250, train_loss = 0.0060, dev_loss = 0.0066)\n",
            "epoch: 7260, dev_loss =  0.0102\n",
            "epoch: 7270, dev_loss =  0.0094\n",
            "epoch: 7280, dev_loss =  0.0075\n",
            "epoch: 7290, dev_loss =  0.0094\n",
            "epoch: 7300, dev_loss =  0.0090\n",
            "epoch: 7310, dev_loss =  0.0071\n",
            "epoch: 7320, dev_loss =  0.0068\n",
            "epoch: 7330, dev_loss =  0.0083\n",
            "epoch: 7340, dev_loss =  0.0086\n",
            "epoch: 7350, dev_loss =  0.0066\n",
            "Saving model (epoch = 7350, train_loss = 0.0060, dev_loss = 0.0066)\n",
            "epoch: 7360, dev_loss =  0.0116\n",
            "epoch: 7370, dev_loss =  0.0083\n",
            "epoch: 7380, dev_loss =  0.0071\n",
            "epoch: 7390, dev_loss =  0.0080\n",
            "epoch: 7400, dev_loss =  0.0075\n",
            "epoch: 7410, dev_loss =  0.0085\n",
            "epoch: 7420, dev_loss =  0.0065\n",
            "Saving model (epoch = 7420, train_loss = 0.0058, dev_loss = 0.0065)\n",
            "epoch: 7430, dev_loss =  0.0069\n",
            "epoch: 7440, dev_loss =  0.0165\n",
            "epoch: 7450, dev_loss =  0.0086\n",
            "epoch: 7460, dev_loss =  0.0096\n",
            "epoch: 7470, dev_loss =  0.0068\n",
            "epoch: 7480, dev_loss =  0.0067\n",
            "epoch: 7490, dev_loss =  0.0079\n",
            "epoch: 7500, dev_loss =  0.0072\n",
            "epoch: 7510, dev_loss =  0.0080\n",
            "epoch: 7520, dev_loss =  0.0098\n",
            "epoch: 7530, dev_loss =  0.0069\n",
            "epoch: 7540, dev_loss =  0.0081\n",
            "epoch: 7550, dev_loss =  0.0065\n",
            "epoch: 7560, dev_loss =  0.0070\n",
            "epoch: 7570, dev_loss =  0.0085\n",
            "epoch: 7580, dev_loss =  0.0073\n",
            "epoch: 7590, dev_loss =  0.0070\n",
            "epoch: 7600, dev_loss =  0.0137\n",
            "epoch: 7610, dev_loss =  0.0086\n",
            "epoch: 7620, dev_loss =  0.0090\n",
            "epoch: 7630, dev_loss =  0.0066\n",
            "epoch: 7640, dev_loss =  0.0068\n",
            "epoch: 7650, dev_loss =  0.0103\n",
            "epoch: 7660, dev_loss =  0.0075\n",
            "epoch: 7670, dev_loss =  0.0083\n",
            "epoch: 7680, dev_loss =  0.0082\n",
            "epoch: 7690, dev_loss =  0.0067\n",
            "epoch: 7700, dev_loss =  0.0071\n",
            "epoch: 7710, dev_loss =  0.0140\n",
            "epoch: 7720, dev_loss =  0.0087\n",
            "epoch: 7730, dev_loss =  0.0086\n",
            "epoch: 7740, dev_loss =  0.0062\n",
            "Saving model (epoch = 7740, train_loss = 0.0059, dev_loss = 0.0062)\n",
            "epoch: 7750, dev_loss =  0.0064\n",
            "epoch: 7760, dev_loss =  0.0087\n",
            "epoch: 7770, dev_loss =  0.0077\n",
            "epoch: 7780, dev_loss =  0.0122\n",
            "epoch: 7790, dev_loss =  0.0072\n",
            "epoch: 7800, dev_loss =  0.0069\n",
            "epoch: 7810, dev_loss =  0.0093\n",
            "epoch: 7820, dev_loss =  0.0066\n",
            "epoch: 7830, dev_loss =  0.0076\n",
            "epoch: 7840, dev_loss =  0.0068\n",
            "epoch: 7850, dev_loss =  0.0065\n",
            "epoch: 7860, dev_loss =  0.0100\n",
            "epoch: 7870, dev_loss =  0.0067\n",
            "epoch: 7880, dev_loss =  0.0066\n",
            "epoch: 7890, dev_loss =  0.0070\n",
            "epoch: 7900, dev_loss =  0.0063\n",
            "epoch: 7910, dev_loss =  0.0065\n",
            "epoch: 7920, dev_loss =  0.0131\n",
            "epoch: 7930, dev_loss =  0.0091\n",
            "epoch: 7940, dev_loss =  0.0069\n",
            "epoch: 7950, dev_loss =  0.0069\n",
            "epoch: 7960, dev_loss =  0.0078\n",
            "epoch: 7970, dev_loss =  0.0082\n",
            "epoch: 7980, dev_loss =  0.0073\n",
            "epoch: 7990, dev_loss =  0.0103\n",
            "epoch: 8000, dev_loss =  0.0070\n",
            "epoch: 8010, dev_loss =  0.0081\n",
            "epoch: 8020, dev_loss =  0.0073\n",
            "epoch: 8030, dev_loss =  0.0065\n",
            "epoch: 8040, dev_loss =  0.0071\n",
            "epoch: 8050, dev_loss =  0.0065\n",
            "epoch: 8060, dev_loss =  0.0077\n",
            "epoch: 8070, dev_loss =  0.0084\n",
            "epoch: 8080, dev_loss =  0.0070\n",
            "epoch: 8090, dev_loss =  0.0098\n",
            "epoch: 8100, dev_loss =  0.0063\n",
            "epoch: 8110, dev_loss =  0.0083\n",
            "epoch: 8120, dev_loss =  0.0072\n",
            "epoch: 8130, dev_loss =  0.0072\n",
            "epoch: 8140, dev_loss =  0.0068\n",
            "epoch: 8150, dev_loss =  0.0079\n",
            "epoch: 8160, dev_loss =  0.0073\n",
            "epoch: 8170, dev_loss =  0.0089\n",
            "epoch: 8180, dev_loss =  0.0075\n",
            "epoch: 8190, dev_loss =  0.0064\n",
            "epoch: 8200, dev_loss =  0.0064\n",
            "epoch: 8210, dev_loss =  0.0061\n",
            "Saving model (epoch = 8210, train_loss = 0.0056, dev_loss = 0.0061)\n",
            "epoch: 8220, dev_loss =  0.0069\n",
            "epoch: 8230, dev_loss =  0.0137\n",
            "epoch: 8240, dev_loss =  0.0075\n",
            "epoch: 8250, dev_loss =  0.0063\n",
            "epoch: 8260, dev_loss =  0.0062\n",
            "epoch: 8270, dev_loss =  0.0065\n",
            "epoch: 8280, dev_loss =  0.0063\n",
            "epoch: 8290, dev_loss =  0.0097\n",
            "epoch: 8300, dev_loss =  0.0063\n",
            "epoch: 8310, dev_loss =  0.0074\n",
            "epoch: 8320, dev_loss =  0.0082\n",
            "epoch: 8330, dev_loss =  0.0068\n",
            "epoch: 8340, dev_loss =  0.0079\n",
            "epoch: 8350, dev_loss =  0.0064\n",
            "epoch: 8360, dev_loss =  0.0078\n",
            "epoch: 8370, dev_loss =  0.0084\n",
            "epoch: 8380, dev_loss =  0.0070\n",
            "epoch: 8390, dev_loss =  0.0062\n",
            "epoch: 8400, dev_loss =  0.0059\n",
            "Saving model (epoch = 8400, train_loss = 0.0055, dev_loss = 0.0059)\n",
            "epoch: 8410, dev_loss =  0.0073\n",
            "epoch: 8420, dev_loss =  0.0061\n",
            "epoch: 8430, dev_loss =  0.0077\n",
            "epoch: 8440, dev_loss =  0.0080\n",
            "epoch: 8450, dev_loss =  0.0066\n",
            "epoch: 8460, dev_loss =  0.0088\n",
            "epoch: 8470, dev_loss =  0.0066\n",
            "epoch: 8480, dev_loss =  0.0060\n",
            "epoch: 8490, dev_loss =  0.0073\n",
            "epoch: 8500, dev_loss =  0.0078\n",
            "epoch: 8510, dev_loss =  0.0073\n",
            "epoch: 8520, dev_loss =  0.0083\n",
            "epoch: 8530, dev_loss =  0.0123\n",
            "epoch: 8540, dev_loss =  0.0080\n",
            "epoch: 8550, dev_loss =  0.0058\n",
            "Saving model (epoch = 8550, train_loss = 0.0054, dev_loss = 0.0058)\n",
            "epoch: 8560, dev_loss =  0.0061\n",
            "epoch: 8570, dev_loss =  0.0139\n",
            "epoch: 8580, dev_loss =  0.0082\n",
            "epoch: 8590, dev_loss =  0.0069\n",
            "epoch: 8600, dev_loss =  0.0070\n",
            "epoch: 8610, dev_loss =  0.0070\n",
            "epoch: 8620, dev_loss =  0.0067\n",
            "epoch: 8630, dev_loss =  0.0068\n",
            "epoch: 8640, dev_loss =  0.0100\n",
            "epoch: 8650, dev_loss =  0.0067\n",
            "epoch: 8660, dev_loss =  0.0077\n",
            "epoch: 8670, dev_loss =  0.0071\n",
            "epoch: 8680, dev_loss =  0.0118\n",
            "epoch: 8690, dev_loss =  0.0074\n",
            "epoch: 8700, dev_loss =  0.0060\n",
            "epoch: 8710, dev_loss =  0.0083\n",
            "epoch: 8720, dev_loss =  0.0063\n",
            "epoch: 8730, dev_loss =  0.0066\n",
            "epoch: 8740, dev_loss =  0.0096\n",
            "epoch: 8750, dev_loss =  0.0084\n",
            "epoch: 8760, dev_loss =  0.0061\n",
            "epoch: 8770, dev_loss =  0.0069\n",
            "epoch: 8780, dev_loss =  0.0056\n",
            "Saving model (epoch = 8780, train_loss = 0.0052, dev_loss = 0.0056)\n",
            "epoch: 8790, dev_loss =  0.0071\n",
            "epoch: 8800, dev_loss =  0.0111\n",
            "epoch: 8810, dev_loss =  0.0086\n",
            "epoch: 8820, dev_loss =  0.0067\n",
            "epoch: 8830, dev_loss =  0.0068\n",
            "epoch: 8840, dev_loss =  0.0112\n",
            "epoch: 8850, dev_loss =  0.0065\n",
            "epoch: 8860, dev_loss =  0.0074\n",
            "epoch: 8870, dev_loss =  0.0067\n",
            "epoch: 8880, dev_loss =  0.0058\n",
            "epoch: 8890, dev_loss =  0.0061\n",
            "epoch: 8900, dev_loss =  0.0146\n",
            "epoch: 8910, dev_loss =  0.0060\n",
            "epoch: 8920, dev_loss =  0.0059\n",
            "epoch: 8930, dev_loss =  0.0061\n",
            "epoch: 8940, dev_loss =  0.0060\n",
            "epoch: 8950, dev_loss =  0.0082\n",
            "epoch: 8960, dev_loss =  0.0079\n",
            "epoch: 8970, dev_loss =  0.0071\n",
            "epoch: 8980, dev_loss =  0.0097\n",
            "epoch: 8990, dev_loss =  0.0083\n",
            "epoch: 9000, dev_loss =  0.0064\n",
            "epoch: 9010, dev_loss =  0.0072\n",
            "epoch: 9020, dev_loss =  0.0060\n",
            "epoch: 9030, dev_loss =  0.0081\n",
            "epoch: 9040, dev_loss =  0.0078\n",
            "epoch: 9050, dev_loss =  0.0090\n",
            "epoch: 9060, dev_loss =  0.0061\n",
            "epoch: 9070, dev_loss =  0.0068\n",
            "epoch: 9080, dev_loss =  0.0058\n",
            "epoch: 9090, dev_loss =  0.0055\n",
            "Saving model (epoch = 9090, train_loss = 0.0052, dev_loss = 0.0055)\n",
            "epoch: 9100, dev_loss =  0.0068\n",
            "epoch: 9110, dev_loss =  0.0139\n",
            "epoch: 9120, dev_loss =  0.0081\n",
            "epoch: 9130, dev_loss =  0.0056\n",
            "epoch: 9140, dev_loss =  0.0059\n",
            "epoch: 9150, dev_loss =  0.0104\n",
            "epoch: 9160, dev_loss =  0.0073\n",
            "epoch: 9170, dev_loss =  0.0057\n",
            "epoch: 9180, dev_loss =  0.0055\n",
            "epoch: 9190, dev_loss =  0.0055\n",
            "Saving model (epoch = 9190, train_loss = 0.0052, dev_loss = 0.0055)\n",
            "epoch: 9200, dev_loss =  0.0193\n",
            "epoch: 9210, dev_loss =  0.0069\n",
            "epoch: 9220, dev_loss =  0.0077\n",
            "epoch: 9230, dev_loss =  0.0066\n",
            "epoch: 9240, dev_loss =  0.0055\n",
            "Saving model (epoch = 9240, train_loss = 0.0053, dev_loss = 0.0055)\n",
            "epoch: 9250, dev_loss =  0.0061\n",
            "epoch: 9260, dev_loss =  0.0067\n",
            "epoch: 9270, dev_loss =  0.0054\n",
            "Saving model (epoch = 9270, train_loss = 0.0050, dev_loss = 0.0054)\n",
            "epoch: 9280, dev_loss =  0.0063\n",
            "epoch: 9290, dev_loss =  0.0115\n",
            "epoch: 9300, dev_loss =  0.0078\n",
            "epoch: 9310, dev_loss =  0.0059\n",
            "epoch: 9320, dev_loss =  0.0058\n",
            "epoch: 9330, dev_loss =  0.0058\n",
            "epoch: 9340, dev_loss =  0.0103\n",
            "epoch: 9350, dev_loss =  0.0064\n",
            "epoch: 9360, dev_loss =  0.0090\n",
            "epoch: 9370, dev_loss =  0.0065\n",
            "epoch: 9380, dev_loss =  0.0079\n",
            "epoch: 9390, dev_loss =  0.0059\n",
            "epoch: 9400, dev_loss =  0.0065\n",
            "epoch: 9410, dev_loss =  0.0070\n",
            "epoch: 9420, dev_loss =  0.0071\n",
            "epoch: 9430, dev_loss =  0.0061\n",
            "epoch: 9440, dev_loss =  0.0066\n",
            "epoch: 9450, dev_loss =  0.0102\n",
            "epoch: 9460, dev_loss =  0.0085\n",
            "epoch: 9470, dev_loss =  0.0056\n",
            "epoch: 9480, dev_loss =  0.0068\n",
            "epoch: 9490, dev_loss =  0.0067\n",
            "epoch: 9500, dev_loss =  0.0059\n",
            "epoch: 9510, dev_loss =  0.0121\n",
            "epoch: 9520, dev_loss =  0.0084\n",
            "epoch: 9530, dev_loss =  0.0065\n",
            "epoch: 9540, dev_loss =  0.0060\n",
            "epoch: 9550, dev_loss =  0.0061\n",
            "epoch: 9560, dev_loss =  0.0075\n",
            "epoch: 9570, dev_loss =  0.0063\n",
            "epoch: 9580, dev_loss =  0.0055\n",
            "epoch: 9590, dev_loss =  0.0112\n",
            "epoch: 9600, dev_loss =  0.0066\n",
            "epoch: 9610, dev_loss =  0.0062\n",
            "epoch: 9620, dev_loss =  0.0086\n",
            "epoch: 9630, dev_loss =  0.0053\n",
            "Saving model (epoch = 9630, train_loss = 0.0050, dev_loss = 0.0053)\n",
            "epoch: 9640, dev_loss =  0.0105\n",
            "epoch: 9650, dev_loss =  0.0081\n",
            "epoch: 9660, dev_loss =  0.0056\n",
            "epoch: 9670, dev_loss =  0.0071\n",
            "epoch: 9680, dev_loss =  0.0063\n",
            "epoch: 9690, dev_loss =  0.0092\n",
            "epoch: 9700, dev_loss =  0.0066\n",
            "epoch: 9710, dev_loss =  0.0066\n",
            "epoch: 9720, dev_loss =  0.0056\n",
            "epoch: 9730, dev_loss =  0.0082\n",
            "epoch: 9740, dev_loss =  0.0087\n",
            "epoch: 9750, dev_loss =  0.0074\n",
            "epoch: 9760, dev_loss =  0.0061\n",
            "epoch: 9770, dev_loss =  0.0141\n",
            "epoch: 9780, dev_loss =  0.0089\n",
            "epoch: 9790, dev_loss =  0.0069\n",
            "epoch: 9800, dev_loss =  0.0054\n",
            "epoch: 9810, dev_loss =  0.0070\n",
            "epoch: 9820, dev_loss =  0.0068\n",
            "epoch: 9830, dev_loss =  0.0057\n",
            "epoch: 9840, dev_loss =  0.0092\n",
            "epoch: 9850, dev_loss =  0.0065\n",
            "epoch: 9860, dev_loss =  0.0060\n",
            "epoch: 9870, dev_loss =  0.0052\n",
            "Saving model (epoch = 9870, train_loss = 0.0048, dev_loss = 0.0052)\n",
            "epoch: 9880, dev_loss =  0.0059\n",
            "epoch: 9890, dev_loss =  0.0137\n",
            "epoch: 9900, dev_loss =  0.0077\n",
            "epoch: 9910, dev_loss =  0.0081\n",
            "epoch: 9920, dev_loss =  0.0054\n",
            "epoch: 9930, dev_loss =  0.0056\n",
            "epoch: 9940, dev_loss =  0.0063\n",
            "epoch: 9950, dev_loss =  0.0080\n",
            "epoch: 9960, dev_loss =  0.0057\n",
            "epoch: 9970, dev_loss =  0.0054\n",
            "epoch: 9980, dev_loss =  0.0055\n",
            "epoch: 9990, dev_loss =  0.0052\n",
            "epoch: 10000, dev_loss =  0.0052\n",
            "Saving model (epoch = 10000, train_loss = 0.0049, dev_loss = 0.0052)\n",
            "Finished training after 10000 epochs\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    # forward pass and loss\n",
        "    y_predicted = model(feature)\n",
        "    # print(y_predicted)\n",
        "    # loss = criterion(y_predicted, target)\n",
        "    mse_loss = model.cal_loss(y_predicted, target)\n",
        "\n",
        "    # backward pass\n",
        "    mse_loss.backward()\n",
        "\n",
        "\n",
        "    # update\n",
        "    optimizer.step()\n",
        "    loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "\n",
        "    dev_mse = dev(feature_dev,target_dev, model, device)\n",
        "    loss_record['dev'].append(dev_mse)\n",
        "\n",
        "    # init optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, dev_loss = {dev_mse.item(): .4f}')\n",
        "        if (dev_mse < min_mse):\n",
        "            torch.save(model.state_dict(), 'models/model.pth')\n",
        "            min_mse = dev_mse\n",
        "            print('Saving model (epoch = {:4d}, train_loss = {:.4f}, dev_loss = {:.4f})'\n",
        "                    .format(epoch + 1, mse_loss, dev_mse))\n",
        "            \n",
        "\n",
        "print('Finished training after {} epochs'.format(epoch+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZFm3bSrZ-F1"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slulcf0CaIFA",
        "outputId": "4c7138be-43a5-49df-e5c4-650826be1c7f"
      },
      "outputs": [],
      "source": [
        "# generate test data\n",
        "# in this problem, the test set is not completely unknown, so I print it out and even calculate the loss\n",
        "x_test, y_test = gen_data(20)\n",
        "test_feature = torch.from_numpy(x_test)\n",
        "test_target = torch.from_numpy(y_test)\n",
        "test_feature,test_target=test_feature.type(torch.FloatTensor),test_target.type(torch.FloatTensor)\n",
        "n_samples, n_features = test_feature.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMFRJxVjcJkc",
        "outputId": "fa810a7a-8e53-412e-8ffb-0849d2ac5965"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del model\n",
        "model = NeuralNet(n_features, n_features).to(device)\n",
        "ckpt = torch.load('models/model.pth', map_location='cpu')  # Load your best model\n",
        "model.load_state_dict(ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxZscuZxZ_wi",
        "outputId": "7fa43723-e1b7-4a52-8d89-0085528495c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_pred: \n",
            "tensor([[ 1.1021, -0.7718,  0.1922],\n",
            "        [-1.2757, -0.2033,  0.6773],\n",
            "        [ 0.3276, -1.1028,  0.1752],\n",
            "        [-0.0795, -0.7841,  1.0956],\n",
            "        [ 0.2495, -0.6925,  1.3186],\n",
            "        [ 0.8539, -0.0747,  1.3080],\n",
            "        [-0.5006, -1.3685,  1.6894],\n",
            "        [ 1.0175, -1.4007,  0.7015],\n",
            "        [-0.6896, -0.1377,  0.1915],\n",
            "        [-0.1786, -0.5474,  0.2046],\n",
            "        [ 1.5404, -0.0146,  0.2690],\n",
            "        [ 0.8975, -0.0731,  0.8508],\n",
            "        [ 0.5911, -1.0351,  0.9964],\n",
            "        [ 0.4391, -0.2024,  0.2970],\n",
            "        [ 1.0838, -0.0997,  0.7971],\n",
            "        [ 0.7721, -1.0606,  1.0419],\n",
            "        [ 0.9076, -1.0742,  0.2213],\n",
            "        [-0.6602, -0.5430,  0.5032],\n",
            "        [ 1.5307, -1.4387,  0.8098],\n",
            "        [ 0.2272, -0.7751,  0.1912]], device='cuda:0')\n",
            "y_test: \n",
            "tensor([[ 1.1179, -0.8195,  0.3079],\n",
            "        [-1.2754, -0.1443,  0.5331],\n",
            "        [ 0.3068, -0.9944, -0.0486],\n",
            "        [-0.0855, -0.7764,  1.1161],\n",
            "        [ 0.2359, -0.6864,  1.3012],\n",
            "        [ 0.8254, -0.0680,  1.2470],\n",
            "        [-0.4789, -1.3826,  1.5657],\n",
            "        [ 1.0232, -1.4295,  0.7233],\n",
            "        [-0.6744, -0.0977,  0.1093],\n",
            "        [-0.1724, -0.5603,  0.2550],\n",
            "        [ 1.5657, -0.0198,  0.2992],\n",
            "        [ 0.8803, -0.0795,  0.8486],\n",
            "        [ 0.5932, -1.0442,  1.0247],\n",
            "        [ 0.4473, -0.1411,  0.2146],\n",
            "        [ 1.0608, -0.0764,  0.7451],\n",
            "        [ 0.7452, -1.1049,  1.0897],\n",
            "        [ 0.9173, -1.1244,  0.3035],\n",
            "        [-0.6410, -0.6232,  0.6647],\n",
            "        [ 1.5641, -1.4798,  0.8444],\n",
            "        [ 0.2373, -0.7576,  0.1756]], device='cuda:0')\n",
            "test MSE loss: \n",
            "tensor(0.0034, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "model.eval()   \n",
        "test_feature = test_feature.to(device) \n",
        "test_target = test_target.to(device)                             # set model to evalutation mode\n",
        "with torch.no_grad():                   # disable gradient calculation\n",
        "    pred = model(test_feature)                     # forward pass (compute output)\n",
        "# print(\"test_feature: \")\n",
        "# print(test_feature)\n",
        "print(\"y_pred: \")\n",
        "print(pred)\n",
        "print(\"y_test: \")\n",
        "print(test_target)\n",
        "print(\"test MSE loss: \")\n",
        "test_loss = model.cal_loss(pred, test_target)\n",
        "print(test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsEnlHlbkbtL"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_learning_curve(loss_record, title='deep model')\n",
        "# the learning curve of dev and train overlaps since this problem is simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnfclAhXnj1h"
      },
      "source": [
        "# Robot Arm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pred\n",
            "[-1.275746  -0.2033295  0.6773192]\n",
            "\n",
            "target\n",
            "tensor([-1.2754, -0.1443,  0.5331], device='cuda:0')\n",
            "\n",
            "joint config\n",
            "[-1.27574599 -0.2033295   0.67731923 -0.47398973  0.        ]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "arr = pred[1].cpu().detach().numpy()\n",
        "if(abs(arr[2])>abs(arr[1])):\n",
        "    t4 = np.sign(arr[1]) * (abs(arr[2])-abs(arr[1]))\n",
        "else:\n",
        "    t4 =  np.sign(arr[2]) * (abs(arr[1])-abs(arr[2]))\n",
        "\n",
        "print('pred')\n",
        "print(arr)\n",
        "print('')\n",
        "\n",
        "print('target')\n",
        "print(test_target[1])\n",
        "print('')\n",
        "\n",
        "arr = np.append(arr, np.array([t4,0]), axis=0)\n",
        "\n",
        "print('joint config')\n",
        "print(arr)\n",
        "print('')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "IC_Project_HW4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
